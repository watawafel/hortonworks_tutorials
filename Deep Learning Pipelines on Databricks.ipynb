{"cells":[{"cell_type":"markdown","source":["# Introducing Deep Learning Pipelines for Apache Spark\n\nDeep Learning Pipelines is a new library published by Databricks to provide high-level APIs for scalable deep learning model application and transfer learning via integration of popular deep learning libraries with MLlib Pipelines and Spark SQL. For an overview and the philosophy behind the library, check out the Databricks [blog post](https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html). This notebook parallels the [Deep Learning Pipelines README](https://github.com/databricks/spark-deep-learning), detailing usage examples with additional tips for getting started with the library on Databricks."],"metadata":{}},{"cell_type":"markdown","source":["## Cluster set-up\n\nDeep Learning Pipelines is available as a Spark Package. To use it on your cluster, create a new library with the Source option \"Maven Coordinate\", using \"Search Spark Packages and Maven Central\" to find \"spark-deep-learning\". Then [attach the library (version 0.2.0) to a cluster](https://docs.databricks.com/user-guide/libraries.html); this notebook is compatible with Deep Learning Pipelines 0.2.0 but not older releases (e.g. 0.1.0). To run this notebook, also create and attach the following libraries: \n* via PyPI: tensorflow, keras, h5py\n* via Spark Packages: tensorframes\n\nDeep Learning Pipelines is compatible with Spark versions 2.0 or higher and works with any instance type (CPU or GPU)."],"metadata":{}},{"cell_type":"markdown","source":["## Quick User Guide\n\nDeep Learning Pipelines provides a suite of tools around working with and processing images using deep learning. The tools can be categorized as\n* **Working with images** natively in Spark DataFrames\n* **Transfer learning**, a super quick way to leverage deep learning\n* **Applying deep learning models at scale**, whether they are your own or known popular models, to image data to make predictions or transform them into features\n* **Deploying models as SQL functions** to empower everyone by making deep learning available in SQL (coming soon)\n* **Distributed hyper-parameter tuning** via Spark MLlib Pipelines (coming soon)\n\nWe'll cover each one with examples below."],"metadata":{}},{"cell_type":"markdown","source":["Let us first get some images to work with in this notebook. We'll use the flowers dataset from the [TensorFlow retraining tutorial](https://www.tensorflow.org/tutorials/image_retraining)."],"metadata":{}},{"cell_type":"code","source":["%sh \ncurl -O http://download.tensorflow.org/example_images/flower_photos.tgz\ntar xzf flower_photos.tgz"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(dbutils.fs.ls('file:/databricks/driver/flower_photos'))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# The 'file:/...' directory will be cleared out upon cluster termination. That doesn't matter for this example notebook, but in most cases we'd want to store the images in a more permanent place. Let's move the files to dbfs so we can see how to work with it in the use cases below.\nimg_dir = '/tmp/flower_photos'\ndbutils.fs.mkdirs(img_dir)\ndbutils.fs.cp('file:/databricks/driver/flower_photos/tulips', img_dir + \"/tulips\", recurse=True)\ndbutils.fs.cp('file:/databricks/driver/flower_photos/daisy', img_dir + \"/daisy\", recurse=True)\ndbutils.fs.cp('file:/databricks/driver/flower_photos/LICENSE.txt', img_dir)\ndisplay(dbutils.fs.ls(img_dir))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Let's create a small sample set of images for quick demonstrations.\nsample_img_dir = img_dir + \"/sample\"\ndbutils.fs.mkdirs(sample_img_dir)\nfiles = dbutils.fs.ls(img_dir + \"/tulips\")[0:1] + dbutils.fs.ls(img_dir + \"/daisy\")[0:2]\nfor f in files:\n  dbutils.fs.cp(f.path, sample_img_dir)\ndisplay(dbutils.fs.ls(sample_img_dir))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Working with images in Spark\n\nThe first step to applying deep learning on images is the ability to load the images. Deep Learning Pipelines includes utility functions that can load millions of images into a Spark DataFrame and decode them automatically in a distributed fashion, allowing manipulationg at scale."],"metadata":{}},{"cell_type":"code","source":["from sparkdl import readImages\nimage_df = readImages(sample_img_dir)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["The resulting DataFrame contains a string column named \"filePath\" containing the path to each image file, and a image struct (\"`SpImage`\") column called \"image\" containing the decoded image data."],"metadata":{}},{"cell_type":"code","source":["display(image_df)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Transfer learning\nDeep Learning Pipelines provides utilities to perform transfer learning on images, which is one of the fastest (code and run-time -wise) ways to start using deep learning. Using Deep Learning Pipelines, it can be done in just several lines of code."],"metadata":{}},{"cell_type":"code","source":["# Create training & test DataFrames for transfer learning - this piece of code is longer than transfer learning itself below!\nfrom sparkdl import readImages\nfrom pyspark.sql.functions import lit\n\ntulips_df = readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(1))\ndaisy_df = readImages(img_dir + \"/daisy\").withColumn(\"label\", lit(0))\ntulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\ndaisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])\ntrain_df = tulips_train.unionAll(daisy_train)\ntest_df = tulips_test.unionAll(daisy_test)\n# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.\n# This ensure that each of the paritions has a small size.\ntrain_df = train_df.repartition(100)\ntest_df = test_df.repartition(100)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom sparkdl import DeepImageFeaturizer \n\nfeaturizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\nlr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\np = Pipeline(stages=[featurizer, lr])\n\np_model = p.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Note: the training step may take a while on Community Edition - try making a smaller training set in that case."],"metadata":{}},{"cell_type":"markdown","source":["Let's see how well the model does:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\ntested_df = p_model.transform(test_df)\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Not bad for a first try with zero tuning! Furthermore, we can look at where we are making mistakes:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import expr\ndef _p1(v):\n  return float(v.array[1])\np1 = udf(_p1, DoubleType())\n\ndf = tested_df.withColumn(\"p_1\", p1(tested_df.probability))\nwrong_df = df.orderBy(expr(\"abs(p_1 - label)\"), ascending=False)\ndisplay(wrong_df.select(\"filePath\", \"p_1\", \"label\").limit(10))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Applying deep learning models at scale\nSpark DataFrames are a natural construct for applying deep learning models to a large-scale dataset. Deep Learning Pipelines provides a set of (Spark MLlib) Transformers for applying TensorFlow Graphs and TensorFlow-backed Keras Models at scale. In addition, popular images models can be applied out of the box, without requiring any TensorFlow or Keras code. The Transformers, backed by the Tensorframes library, efficiently handle the distribution of models and data to Spark workers."],"metadata":{}},{"cell_type":"markdown","source":["#### Applying popular image models\nThere are many well-known deep learning models for images. If the task at hand is very similar to what the models provide (e.g. object recognition with ImageNet classes), or for pure exploration, one can use the Transformer `DeepImagePredictor` by simply specifying the model name."],"metadata":{}},{"cell_type":"code","source":["from sparkdl import readImages, DeepImagePredictor\n\nimage_df = readImages(sample_img_dir)\n\npredictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\npredictions_df = predictor.transform(image_df)\n\ndisplay(predictions_df.select(\"filePath\", \"predicted_labels\"))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Notice that the `predicted_labels` column shows \"daisy\" as a high probability class for all sample flowers using this base model. However, as can be seen from the differences in the probability values, the neural network has the information to discern the two flower types. Hence our transfer learning example above was able to properly learn the differences between daisies and tulips starting from the base model."],"metadata":{}},{"cell_type":"code","source":["df = p_model.transform(image_df)\ndisplay(df.select(\"filePath\", (1-p1(df.probability)).alias(\"p_daisy\")))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#### For TensorFlow users\nDeep Learning Pipelines provides a MLlib Transformer that will apply the given TensorFlow Graph to a DataFrame containing a column of images (e.g. loaded using the utilities described in the previous section). Here is a very simple example of how a TensorFlow Graph can be used with the Transformer. In practice, the TensorFlow Graph will likely be restored from files before calling `TFImageTransformer`."],"metadata":{}},{"cell_type":"code","source":["from sparkdl import readImages, TFImageTransformer\nimport sparkdl.graph.utils as tfx\nfrom sparkdl.transformers import utils\nimport tensorflow as tf\n\ngraph = tf.Graph()\nwith tf.Session(graph=graph) as sess:\n    image_arr = utils.imageInputPlaceholder()\n    resized_images = tf.image.resize_images(image_arr, (299, 299))\n    frozen_graph = tfx.strip_and_freeze_until([resized_images], graph, sess,\n                                              return_graph=True)\n\ntransformer = TFImageTransformer(inputCol=\"image\", outputCol=\"predictions\", graph=frozen_graph,\n                                 inputTensor=image_arr, outputTensor=resized_images,\n                                 outputMode=\"image\")\nimage_df = readImages(sample_img_dir)\nprocessed_image_df = transformer.transform(image_df)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### For Keras users\nFor applying Keras models in a distributed manner using Spark, [`KerasImageFileTransformer`](link_here) works on TensorFlow-backed Keras models. It \n* Internally creates a DataFrame containing a column of images by applying the user-specified image loading and processing function to the input DataFrame containing a column of image URIs\n* Loads a Keras model from the given model file path \n* Applies the model to the image DataFrame\n\nThe difference in the API from `TFImageTransformer` above stems from the fact that usual Keras workflows have very specific ways to load and resize images that are not part of the TensorFlow Graph."],"metadata":{}},{"cell_type":"markdown","source":["To use the transformer, we first need to have a Keras model stored as a file. For this notebook we'll just save the Keras built-in InceptionV3 model instead of training one."],"metadata":{}},{"cell_type":"code","source":["from keras.applications import InceptionV3\n\nmodel = InceptionV3(weights=\"imagenet\")\nmodel.save('/tmp/model-full.h5')  # saves to the local filesystem\n# move to a permanent place for future use\ndbfs_model_path = 'dbfs:/models/model-full.h5'\ndbutils.fs.cp('file:/tmp/model-full.h5', dbfs_model_path)  "],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Now on the prediction side:"],"metadata":{}},{"cell_type":"code","source":["from keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.image import img_to_array, load_img\nimport numpy as np\nfrom pyspark.sql.types import StringType\nfrom sparkdl import KerasImageFileTransformer\n\ndef loadAndPreprocessKerasInceptionV3(uri):\n  # this is a typical way to load and prep images in keras\n  image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n  image = np.expand_dims(image, axis=0)\n  return preprocess_input(image)\n\ndbutils.fs.cp(dbfs_model_path, 'file:/tmp/model-full-tmp.h5')\ntransformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n                                        modelFile='/tmp/model-full-tmp.h5',  # local file path for model\n                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n                                        outputMode=\"vector\")\n\nfiles = [\"/dbfs\" + str(f.path)[5:] for f in dbutils.fs.ls(sample_img_dir)]  # make \"local\" file paths for images\nuri_df = sqlContext.createDataFrame(files, StringType()).toDF(\"uri\")\n\nkeras_pred_df = transformer.transform(uri_df)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(keras_pred_df.select(\"uri\", \"predictions\"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["#### Clean up data generated for this notebook"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(img_dir, recurse=True)\ndbutils.fs.rm(dbfs_model_path)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["### Resources\n* See the Databricks [blog post](https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html) announcing Deep Learning Pipelines for a high-level overview and more in-depth discussion of some of the concepts here.\n* Check out the [Deep Learning Pipelines github page](https://github.com/databricks/spark-deep-learning).\n* Learn more about [deep learning on Databricks](https://docs.databricks.com/applications/deep-learning/index.html)."],"metadata":{}}],"metadata":{"name":"Deep Learning Pipelines on Databricks","notebookId":1085598326244076},"nbformat":4,"nbformat_minor":0}
